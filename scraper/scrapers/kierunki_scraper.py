from bs4 import BeautifulSoup
from scraper.utils import fetch_page

BASE_URL = "https://plan.uz.zgora.pl/"


def scrape_kierunki() -> list[dict]:
    """
    Pobiera i zwraca listƒô kierunk√≥w studi√≥w z g≈Ç√≥wnej strony planu.
    Zwraca listƒô s≈Çownik√≥w: {'nazwa': ..., 'wydzial': ..., 'link_strony_kierunku': ...}
    """
    URL = BASE_URL + "grupy_lista_kierunkow.php"
    print(f"üîç Pobieram dane z: {URL}")
    html = fetch_page(URL)
    if not html:
        print("‚ùå Nie uda≈Ço siƒô pobraƒá strony z listƒÖ kierunk√≥w.")
        return []
    return parse_departments_and_courses(html)


def parse_departments_and_courses(html: str) -> list[dict]:
    soup = BeautifulSoup(html, "html.parser")
    container = soup.find("div", class_="container main")
    if not container:
        print("‚ùå Nie znaleziono g≈Ç√≥wnego kontenera.")
        return []
    kierunki = []
    current_wydzial = None
    for element in container.find_all("li", class_="lista-grup-item"):
        sub_ul = element.find("ul", class_="lista-grup")
        if sub_ul:
            # To jest nag≈Ç√≥wek wydzia≈Çu
            current_wydzial = element.contents[0].strip()
            continue
        anchor = element.find("a", href=True)
        # Pomijaj, je≈õli nie ma linku lub nie ma tekstu (czyli nie jest to kierunek)
        if not anchor or "ID=" not in anchor['href'] or not anchor.text.strip():
            continue
        nazwa_kierunku = anchor.text.strip()
        link_strony_kierunku = BASE_URL + anchor['href'] if not anchor['href'].startswith('http') else anchor['href']
        kierunki.append({
            "nazwa": nazwa_kierunku,
            "wydzial": current_wydzial,
            "link_strony_kierunku": link_strony_kierunku
        })
    # Ostateczny filtr na wszelki wypadek
    kierunki = [k for k in kierunki if k.get('nazwa') and k.get('nazwa').strip()]
    return kierunki
